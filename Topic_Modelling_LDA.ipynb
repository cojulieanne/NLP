{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic Modelling Basic",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/perkykooky/NLP/blob/main/Topic_Modelling_Basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wymSn7-xc5gR"
      },
      "source": [
        "### Topic Modelling example for News Headlines \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G44dH3mydbs4"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6BG9QXA7xYT"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0SZRKDsjx_G",
        "outputId": "42bcfb47-2b63-4820-8e8e-b51ddde79af7"
      },
      "source": [
        "!kaggle datasets download -d therohk/million-headlines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/kaggle.json'\n",
            "Downloading million-headlines.zip to /content\n",
            " 43% 9.00M/21.1M [00:00<00:00, 44.0MB/s]\n",
            "100% 21.1M/21.1M [00:00<00:00, 70.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHmRPFu98s1d",
        "outputId": "1d6abbbb-de7d-4b8a-eafa-43db3fbe906f"
      },
      "source": [
        "!unzip \\*.zip && rm *.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  million-headlines.zip\n",
            "  inflating: abcnews-date-text.csv   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAkquRXwoXAy"
      },
      "source": [
        "Data contains 18 years worth of headlines from Australian news company (Australian Broadcasting Corporation)\n",
        "\n",
        "**Schema:** \n",
        "  <br> publish_date (YYYYMMDD)\n",
        "  <br> headline_text (string)\n",
        "\n",
        "**Date Range:** [2003-02-19, 2020-12-31]\n",
        "\n",
        "**Size:** (1226258, 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "T4VjJF8JktTS",
        "outputId": "434a6f42-bf3a-49c0-afe7-9fc0a691fad1"
      },
      "source": [
        "data = pd.read_csv('abcnews-date-text.csv')\n",
        "data.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>publish_date</th>\n",
              "      <th>headline_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20030219</td>\n",
              "      <td>aba decides against community broadcasting lic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20030219</td>\n",
              "      <td>act fire witnesses must be aware of defamation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20030219</td>\n",
              "      <td>a g calls for infrastructure protection summit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20030219</td>\n",
              "      <td>air nz staff in aust strike for pay rise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20030219</td>\n",
              "      <td>air nz strike to affect australian travellers</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   publish_date                                      headline_text\n",
              "0      20030219  aba decides against community broadcasting lic...\n",
              "1      20030219     act fire witnesses must be aware of defamation\n",
              "2      20030219     a g calls for infrastructure protection summit\n",
              "3      20030219           air nz staff in aust strike for pay rise\n",
              "4      20030219      air nz strike to affect australian travellers"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "f8ZdPieBcz_E",
        "outputId": "1ef1455b-9357-43a9-bb35-8002751c7958"
      },
      "source": [
        "documents = data['headline_text'].reset_index()\n",
        "documents.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>headline_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>aba decides against community broadcasting lic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>act fire witnesses must be aware of defamation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>a g calls for infrastructure protection summit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>air nz staff in aust strike for pay rise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>air nz strike to affect australian travellers</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index                                      headline_text\n",
              "0      0  aba decides against community broadcasting lic...\n",
              "1      1     act fire witnesses must be aware of defamation\n",
              "2      2     a g calls for infrastructure protection summit\n",
              "3      3           air nz staff in aust strike for pay rise\n",
              "4      4      air nz strike to affect australian travellers"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GOui2O5cz_F"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "  * Splitting the text into sentences and then into words.\n",
        "  * Cleaning any uunnecessary non-alphanumeric characters.\n",
        "  * Lowercase all strings.\n",
        "  * Removing articles, stopwords and other noise (less than 3 characters)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWPEQdjLcz_G",
        "outputId": "113c4ba2-d034-4456-fd97-97c4bdab9217"
      },
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE2nAbXscz_H",
        "outputId": "99e1e7d4-6bdc-4710-9202-f6d8315e2082"
      },
      "source": [
        "print(STOPWORDS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "frozenset({'which', 'with', 'you', 'does', 'see', 'whence', 'couldnt', 'really', 'etc', 'until', 'describe', 'latterly', 'hers', 'became', 'by', 'out', 'up', 'before', 'back', 'thin', 'thereby', 'least', 'fire', 'somehow', 'across', 'sometimes', 'is', 'nobody', 'just', 'such', 'give', 'indeed', 'due', 'noone', 'two', 'didn', 'without', 'whereafter', 'three', 'beforehand', 're', 'might', 'whither', 'themselves', 'although', 'if', 'first', 'against', 'bottom', 'while', 'bill', 'must', 'whereas', 'whole', 'call', 'she', 'used', 'our', 'nor', 'your', 'hence', 'both', 'each', 'whoever', 'per', 'put', 'now', 'therein', 'everyone', 'behind', 'anyhow', 'eight', 'off', 'kg', 'become', 'always', 'can', 'someone', 'onto', 'than', 'more', 'ourselves', 'below', 'he', 'un', 'either', 'few', 'beyond', 'nevertheless', 'would', 'fill', 'had', 'all', 'another', 'hereupon', 'their', 'de', 'third', 'they', 'itself', 'hasnt', 'the', 'that', 'himself', 'me', 'about', 'along', 'during', 'besides', 'will', 'nothing', 'found', 'was', 'cannot', 'mostly', 'above', 'then', 'other', 'never', 'otherwise', 'km', 'through', 'formerly', 'don', 'done', 'to', 'whatever', 'same', 'amount', 'her', 'down', 'but', 'anyone', 'once', 'nine', 'seems', 'elsewhere', 'an', 'eleven', 'sometime', 'ltd', 'thereafter', 'one', 'go', 'be', 'sincere', 'a', 'everything', 'here', 'these', 'thru', 'in', 'thus', 'con', 'only', 'myself', 'move', 'empty', 'show', 'within', 'who', 'serious', 'latter', 'yours', 'most', 'fifteen', 'ie', 'mill', 'part', 'though', 'between', 'anyway', 'could', 'find', 'yourself', 'some', 'yourselves', 'into', 'perhaps', 'being', 'namely', 'we', 'various', 'doing', 'after', 'sixty', 'i', 'am', 'however', 'been', 'rather', 'hereby', 'top', 'its', 'neither', 'amongst', 'anywhere', 'upon', 'somewhere', 'herself', 'when', 'last', 'why', 'even', 'whereby', 'ever', 'further', 'keep', 'for', 'how', 'beside', 'at', 'so', 'were', 'thence', 'using', 'under', 'around', 'not', 'do', 'five', 'them', 'fify', 'since', 'because', 'nowhere', 'say', 'becoming', 'any', 'too', 'everywhere', 'my', 'what', 'well', 'wherein', 'ten', 'from', 'whether', 'whenever', 'should', 'full', 'whose', 'cant', 'own', 'throughout', 'former', 'whom', 'often', 'toward', 'anything', 'every', 'his', 'take', 'please', 'thick', 'system', 'thereupon', 'and', 'did', 'together', 'ours', 'very', 'others', 'forty', 'may', 'among', 'him', 'still', 'next', 'or', 'front', 'wherever', 'eg', 'moreover', 'none', 'co', 'no', 'already', 'quite', 'towards', 'whereupon', 'yet', 'made', 'over', 'herein', 'inc', 'seemed', 'on', 'it', 'enough', 'again', 'as', 'afterwards', 'mine', 'us', 'much', 'twenty', 'via', 'amoungst', 'unless', 'name', 'those', 'there', 'almost', 'twelve', 'becomes', 'many', 'get', 'meanwhile', 'something', 'cry', 'seem', 'detail', 'doesn', 'six', 'hereafter', 'therefore', 'four', 'side', 'regarding', 'hundred', 'where', 'of', 'computer', 'has', 'several', 'also', 'else', 'less', 'alone', 'interest', 'seeming', 'make', 'except', 'are', 'this', 'have'})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWVqN3hJcz_H"
      },
      "source": [
        "# Stemming and Lemmatizing\n",
        "\n",
        "def lemmatize_stemming(text):\n",
        "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
        "\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in STOPWORDS and len(token) >3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7-eKnxNcz_I",
        "outputId": "0688424d-6770-41a2-d476-4a690bd86116"
      },
      "source": [
        "# Sample Output for preprocess()\n",
        "\n",
        "sample = documents.values[0][1]\n",
        "print(\"All Tokens: {}\\n\".format(sample.split(' ')))\n",
        "print(\"Preprocessed and Relevant Tokens: {}\".format(preprocess(sample)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All Tokens: ['aba', 'decides', 'against', 'community', 'broadcasting', 'licence']\n",
            "\n",
            "Preprocessed and Relevant Tokens: ['decide', 'community', 'broadcast', 'licence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "PJt58xMGcz_J",
        "outputId": "d855a62e-7292-4af0-b806-618830d156ad"
      },
      "source": [
        "# Insert new column for preprocessed tokens\n",
        "\n",
        "documents['preprocessed'] = documents['headline_text'].apply(lambda x: preprocess(x))\n",
        "documents.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>headline_text</th>\n",
              "      <th>preprocessed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>aba decides against community broadcasting lic...</td>\n",
              "      <td>[decide, community, broadcast, licence]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>act fire witnesses must be aware of defamation</td>\n",
              "      <td>[witness, aware, defamation]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>a g calls for infrastructure protection summit</td>\n",
              "      <td>[call, infrastructure, protection, summit]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>air nz staff in aust strike for pay rise</td>\n",
              "      <td>[staff, aust, strike, rise]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>air nz strike to affect australian travellers</td>\n",
              "      <td>[strike, affect, australian, travellers]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  ...                                preprocessed\n",
              "0      0  ...     [decide, community, broadcast, licence]\n",
              "1      1  ...                [witness, aware, defamation]\n",
              "2      2  ...  [call, infrastructure, protection, summit]\n",
              "3      3  ...                 [staff, aust, strike, rise]\n",
              "4      4  ...    [strike, affect, australian, travellers]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doS4HVvgcz_L"
      },
      "source": [
        "# Creating a dictionary of all unique words with a unique integer id\n",
        "\n",
        "dictionary = gensim.corpora.Dictionary(documents['preprocessed'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PZT2Wf5tZ7H",
        "outputId": "ce16d034-a6ad-4356-83fc-6cd794a0efea"
      },
      "source": [
        "# Accessing values in the gensim dictionary\n",
        "from pprint import pprint\n",
        "\n",
        "sample = documents['preprocessed'].values[200] #change indices to check other values\n",
        "id = dictionary.token2id[sample[0]]\n",
        "\n",
        "\n",
        "print(sample)\n",
        "print(\"\\nWord \\\"{}\\\" has key {} in the dictionary.\\n\".format(sample[0], dictionary.token2id[sample[0]]))\n",
        "print(\"Word \\\"{}\\\" appeared in {} documents.\\n\".format(sample[0], dictionary.dfs[dictionary.token2id[sample[0]]]))\n",
        "print(\"Total of {} documents processed.\\n\".format(dictionary.num_docs))\n",
        "print(\"Total of {} words processed.\\n\".format(dictionary.num_pos))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['academic', 'upbeat', 'higher', 'education', 'review']\n",
            "\n",
            "Word \"academic\" has key 660 in the dictionary.\n",
            "\n",
            "Word \"academic\" appeared in 513 documents.\n",
            "\n",
            "Total of 1226258 documents processed.\n",
            "\n",
            "Total of 5726030 words processed.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSvZh9nIcz_L",
        "outputId": "f45d64dc-3004-47c1-fa11-0c04ee3b51b7"
      },
      "source": [
        "# Enumerating values in the dictionary\n",
        "\n",
        "count = 0\n",
        "for k,v in dictionary.iteritems():\n",
        "    print(k,v)\n",
        "    count +=1\n",
        "    if count > 10:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 broadcast\n",
            "1 community\n",
            "2 decide\n",
            "3 licence\n",
            "4 aware\n",
            "5 defamation\n",
            "6 witness\n",
            "7 call\n",
            "8 infrastructure\n",
            "9 protection\n",
            "10 summit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXPC4dSlcz_M"
      },
      "source": [
        "# Filtering tokens based on their DF (document frequency)\n",
        "# no_below: minimum number of appearance in x documents\n",
        "# no_above: fraction of all documents (max)\n",
        "# keep_n: keep first n most frequent words\n",
        "\n",
        "dictionary.filter_extremes(no_below = 15, no_above = 0.5, keep_n = 100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5G9PURPUjhP"
      },
      "source": [
        "# **Creating the Bag of Words Matrix**\n",
        "\n",
        "The Bag of Words Matrix simply calculates the **Term Frequency** of a token in the document it belongs to. It is the TF part of the TF-IDF matrix with no weightage on the importance of the token in the collection of documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txvuAgzhcz_M"
      },
      "source": [
        "# Convert each list of tokens per row into BOW format \n",
        "# (int, int) -> (integer id of token in dict, count of token)\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in documents['preprocessed']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ux8IOEducz_M",
        "outputId": "13376b67-c383-4dd4-f09b-afd20f870eb8"
      },
      "source": [
        "bow_corpus[100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(78, 1), (362, 1), (363, 1)]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y7NKKm4cz_P",
        "outputId": "12f586e8-a3fc-4603-b335-548635aa45d4"
      },
      "source": [
        "bow_sample = bow_corpus[100]\n",
        "\n",
        "for i in range(len(bow_sample)):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time in new document.\".format(bow_sample[i][0], dictionary[bow_sample[i][0]], bow_sample[i][1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word 78 (\"urge\") appears 1 time in new document.\n",
            "Word 362 (\"councillors\") appears 1 time in new document.\n",
            "Word 363 (\"women\") appears 1 time in new document.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIzrEGM2Bp0E"
      },
      "source": [
        "#**Creating the TF-IDF Matrix**\n",
        "\n",
        "The TF-IDF Matrix calculates the relevance and importance of a token in a document within a collectin of documents.\n",
        "\n",
        "It is based off of 2 measures:\n",
        "*   **Term Frequency:** count of instances of a token in a document\n",
        "*   **Inverse Document Frequency:** log inverse of fraction of documents token appears in \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSgCFNyacz_P",
        "outputId": "5d9db059-3310-45c5-b651-ff3e8852ccd1"
      },
      "source": [
        "from gensim import corpora, models\n",
        "\n",
        "tfidf = models.TfidfModel(bow_corpus)\n",
        "corpus_tfidf = tfidf[bow_corpus]\n",
        "\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "# get tfidf vector representation of first entry\n",
        "\n",
        "for doc in corpus_tfidf:\n",
        "    pprint(doc)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, 0.5918674193999763),\n",
            " (1, 0.3937180767686992),\n",
            " (2, 0.5009876624450964),\n",
            " (3, 0.49365007440105513)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-zfOMmRWrbn"
      },
      "source": [
        "## **Latent Dirichlet Allocation on Bag of Words Matrix**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ub32LyCocz_P"
      },
      "source": [
        "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = 10, id2word = dictionary, passes=2, workers =2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HoPZjCH1cz_Q",
        "outputId": "f03dcca4-7824-40b2-bccd-cb67ca03fc2c"
      },
      "source": [
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.040*\"queensland\" + 0.023*\"test\" + 0.016*\"australia\" + 0.010*\"game\" + 0.009*\"season\" + 0.009*\"northern\" + 0.009*\"john\" + 0.008*\"black\" + 0.008*\"coronavirus\" + 0.008*\"city\"\n",
            "Topic: 1 \n",
            "Words: 0.047*\"trump\" + 0.038*\"sydney\" + 0.020*\"open\" + 0.016*\"coronavirus\" + 0.014*\"hospital\" + 0.013*\"victorian\" + 0.013*\"speak\" + 0.012*\"care\" + 0.012*\"interview\" + 0.010*\"age\"\n",
            "Topic: 2 \n",
            "Words: 0.046*\"australian\" + 0.033*\"case\" + 0.032*\"court\" + 0.021*\"face\" + 0.017*\"people\" + 0.013*\"morrison\" + 0.012*\"tell\" + 0.012*\"release\" + 0.011*\"hear\" + 0.011*\"rule\"\n",
            "Topic: 3 \n",
            "Words: 0.025*\"government\" + 0.018*\"health\" + 0.017*\"school\" + 0.016*\"state\" + 0.014*\"say\" + 0.012*\"call\" + 0.012*\"federal\" + 0.011*\"indigenous\" + 0.010*\"election\" + 0.010*\"labor\"\n",
            "Topic: 4 \n",
            "Words: 0.037*\"australia\" + 0.023*\"news\" + 0.013*\"protest\" + 0.012*\"scott\" + 0.010*\"country\" + 0.010*\"darwin\" + 0.010*\"beat\" + 0.009*\"president\" + 0.008*\"south\" + 0.007*\"mark\"\n",
            "Topic: 5 \n",
            "Words: 0.025*\"live\" + 0.020*\"die\" + 0.017*\"north\" + 0.017*\"crash\" + 0.016*\"south\" + 0.013*\"royal\" + 0.012*\"west\" + 0.012*\"border\" + 0.011*\"australia\" + 0.011*\"farm\"\n",
            "Topic: 6 \n",
            "Words: 0.048*\"police\" + 0.021*\"charge\" + 0.020*\"death\" + 0.016*\"murder\" + 0.015*\"attack\" + 0.015*\"kill\" + 0.015*\"woman\" + 0.014*\"years\" + 0.013*\"shoot\" + 0.013*\"jail\"\n",
            "Topic: 7 \n",
            "Words: 0.060*\"coronavirus\" + 0.029*\"covid\" + 0.022*\"victoria\" + 0.015*\"market\" + 0.012*\"tasmania\" + 0.011*\"restrictions\" + 0.011*\"rise\" + 0.010*\"record\" + 0.008*\"tasmanian\" + 0.008*\"break\"\n",
            "Topic: 8 \n",
            "Words: 0.018*\"bushfire\" + 0.017*\"work\" + 0.017*\"trial\" + 0.013*\"guilty\" + 0.013*\"rural\" + 0.009*\"video\" + 0.009*\"future\" + 0.009*\"western\" + 0.009*\"help\" + 0.008*\"peter\"\n",
            "Topic: 9 \n",
            "Words: 0.028*\"donald\" + 0.022*\"coast\" + 0.016*\"miss\" + 0.016*\"world\" + 0.016*\"national\" + 0.014*\"life\" + 0.014*\"change\" + 0.014*\"gold\" + 0.012*\"drum\" + 0.011*\"park\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fET0ak8o6x5k"
      },
      "source": [
        "# **Latent Dirichlet Allocation on TF-IDF Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pInmCwxz66se"
      },
      "source": [
        "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics = 10, id2word = dictionary, passes=4, workers =4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GSIG-PUTcz_Q",
        "outputId": "bbeab4e5-6ad9-48b3-d2da-63724118b4ad"
      },
      "source": [
        "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
        "    print('Topic: {} Word: {}'.format(idx, topic))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic: 0 Word: 0.015*\"charge\" + 0.014*\"murder\" + 0.010*\"court\" + 0.010*\"child\" + 0.009*\"assault\" + 0.009*\"royal\" + 0.008*\"guilty\" + 0.008*\"abuse\" + 0.008*\"sentence\" + 0.008*\"police\"\n",
            "Topic: 1 Word: 0.015*\"crash\" + 0.014*\"police\" + 0.010*\"kill\" + 0.009*\"die\" + 0.009*\"shoot\" + 0.008*\"dead\" + 0.008*\"search\" + 0.008*\"woman\" + 0.007*\"hour\" + 0.007*\"miss\"\n",
            "Topic: 2 Word: 0.010*\"government\" + 0.008*\"border\" + 0.007*\"restrictions\" + 0.006*\"coronavirus\" + 0.006*\"korea\" + 0.006*\"cattle\" + 0.005*\"say\" + 0.005*\"action\" + 0.004*\"china\" + 0.004*\"minister\"\n",
            "Topic: 3 Word: 0.027*\"trump\" + 0.008*\"bushfire\" + 0.006*\"age\" + 0.006*\"country\" + 0.006*\"flood\" + 0.006*\"south\" + 0.006*\"rain\" + 0.006*\"storm\" + 0.005*\"coast\" + 0.005*\"bushfires\"\n",
            "Topic: 4 Word: 0.020*\"news\" + 0.013*\"market\" + 0.012*\"rural\" + 0.008*\"monday\" + 0.007*\"john\" + 0.007*\"wall\" + 0.007*\"national\" + 0.006*\"business\" + 0.006*\"street\" + 0.006*\"share\"\n",
            "Topic: 5 Word: 0.026*\"covid\" + 0.025*\"coronavirus\" + 0.008*\"climate\" + 0.007*\"case\" + 0.006*\"change\" + 0.005*\"august\" + 0.005*\"victoria\" + 0.005*\"quarantine\" + 0.005*\"australia\" + 0.005*\"daniel\"\n",
            "Topic: 6 Word: 0.011*\"interview\" + 0.009*\"world\" + 0.008*\"australia\" + 0.008*\"scott\" + 0.008*\"final\" + 0.007*\"morrison\" + 0.007*\"league\" + 0.007*\"michael\" + 0.006*\"david\" + 0.006*\"cricket\"\n",
            "Topic: 7 Word: 0.014*\"drum\" + 0.006*\"stories\" + 0.006*\"hill\" + 0.005*\"hong\" + 0.005*\"territory\" + 0.005*\"footage\" + 0.005*\"kong\" + 0.004*\"march\" + 0.004*\"drive\" + 0.004*\"whale\"\n",
            "Topic: 8 Word: 0.018*\"donald\" + 0.009*\"friday\" + 0.009*\"wednesday\" + 0.007*\"weather\" + 0.007*\"turnbull\" + 0.007*\"peter\" + 0.007*\"lockdown\" + 0.006*\"brief\" + 0.006*\"history\" + 0.005*\"smith\"\n",
            "Topic: 9 Word: 0.007*\"health\" + 0.007*\"queensland\" + 0.006*\"fund\" + 0.006*\"election\" + 0.006*\"tuesday\" + 0.006*\"coronavirus\" + 0.006*\"federal\" + 0.005*\"budget\" + 0.005*\"update\" + 0.005*\"plan\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3Jsecgcicz_Q"
      },
      "source": [
        "test = bow_corpus[1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dICjmnWBcz_Q",
        "outputId": "76a6dcd0-41d3-42ad-886a-bbf81dfa6ed5"
      },
      "source": [
        "for index, score in sorted(lda_model[test], key=lambda tup: -1*tup[1]):\n",
        "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Score: 0.6781476736068726\t \n",
            "Topic: 0.060*\"coronavirus\" + 0.029*\"covid\" + 0.022*\"victoria\" + 0.015*\"market\" + 0.012*\"tasmania\" + 0.011*\"restrictions\" + 0.011*\"rise\" + 0.010*\"record\" + 0.008*\"tasmanian\" + 0.008*\"break\"\n",
            "\n",
            "Score: 0.1885150521993637\t \n",
            "Topic: 0.048*\"police\" + 0.021*\"charge\" + 0.020*\"death\" + 0.016*\"murder\" + 0.015*\"attack\" + 0.015*\"kill\" + 0.015*\"woman\" + 0.014*\"years\" + 0.013*\"shoot\" + 0.013*\"jail\"\n",
            "\n",
            "Score: 0.016667615622282028\t \n",
            "Topic: 0.028*\"donald\" + 0.022*\"coast\" + 0.016*\"miss\" + 0.016*\"world\" + 0.016*\"national\" + 0.014*\"life\" + 0.014*\"change\" + 0.014*\"gold\" + 0.012*\"drum\" + 0.011*\"park\"\n",
            "\n",
            "Score: 0.016667520627379417\t \n",
            "Topic: 0.040*\"queensland\" + 0.023*\"test\" + 0.016*\"australia\" + 0.010*\"game\" + 0.009*\"season\" + 0.009*\"northern\" + 0.009*\"john\" + 0.008*\"black\" + 0.008*\"coronavirus\" + 0.008*\"city\"\n",
            "\n",
            "Score: 0.016667397692799568\t \n",
            "Topic: 0.025*\"government\" + 0.018*\"health\" + 0.017*\"school\" + 0.016*\"state\" + 0.014*\"say\" + 0.012*\"call\" + 0.012*\"federal\" + 0.011*\"indigenous\" + 0.010*\"election\" + 0.010*\"labor\"\n",
            "\n",
            "Score: 0.016667375341057777\t \n",
            "Topic: 0.037*\"australia\" + 0.023*\"news\" + 0.013*\"protest\" + 0.012*\"scott\" + 0.010*\"country\" + 0.010*\"darwin\" + 0.010*\"beat\" + 0.009*\"president\" + 0.008*\"south\" + 0.007*\"mark\"\n",
            "\n",
            "Score: 0.01666710525751114\t \n",
            "Topic: 0.046*\"australian\" + 0.033*\"case\" + 0.032*\"court\" + 0.021*\"face\" + 0.017*\"people\" + 0.013*\"morrison\" + 0.012*\"tell\" + 0.012*\"release\" + 0.011*\"hear\" + 0.011*\"rule\"\n",
            "\n",
            "Score: 0.01666690967977047\t \n",
            "Topic: 0.047*\"trump\" + 0.038*\"sydney\" + 0.020*\"open\" + 0.016*\"coronavirus\" + 0.014*\"hospital\" + 0.013*\"victorian\" + 0.013*\"speak\" + 0.012*\"care\" + 0.012*\"interview\" + 0.010*\"age\"\n",
            "\n",
            "Score: 0.01666666567325592\t \n",
            "Topic: 0.025*\"live\" + 0.020*\"die\" + 0.017*\"north\" + 0.017*\"crash\" + 0.016*\"south\" + 0.013*\"royal\" + 0.012*\"west\" + 0.012*\"border\" + 0.011*\"australia\" + 0.011*\"farm\"\n",
            "\n",
            "Score: 0.01666666567325592\t \n",
            "Topic: 0.018*\"bushfire\" + 0.017*\"work\" + 0.017*\"trial\" + 0.013*\"guilty\" + 0.013*\"rural\" + 0.009*\"video\" + 0.009*\"future\" + 0.009*\"western\" + 0.009*\"help\" + 0.008*\"peter\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DN8sQzZWcz_Q",
        "outputId": "e44fdc07-3d9c-4f42-cbcd-460837877fa2"
      },
      "source": [
        "for index, score in sorted(lda_model_tfidf[test], key=lambda tup: -1*tup[1]):\n",
        "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Score: 0.4290045499801636\t \n",
            "Topic: 0.015*\"crash\" + 0.014*\"police\" + 0.010*\"kill\" + 0.009*\"die\" + 0.009*\"shoot\" + 0.008*\"dead\" + 0.008*\"search\" + 0.008*\"woman\" + 0.007*\"hour\" + 0.007*\"miss\"\n",
            "\n",
            "Score: 0.23570944368839264\t \n",
            "Topic: 0.015*\"charge\" + 0.014*\"murder\" + 0.010*\"court\" + 0.010*\"child\" + 0.009*\"assault\" + 0.009*\"royal\" + 0.008*\"guilty\" + 0.008*\"abuse\" + 0.008*\"sentence\" + 0.008*\"police\"\n",
            "\n",
            "Score: 0.21859821677207947\t \n",
            "Topic: 0.020*\"news\" + 0.013*\"market\" + 0.012*\"rural\" + 0.008*\"monday\" + 0.007*\"john\" + 0.007*\"wall\" + 0.007*\"national\" + 0.006*\"business\" + 0.006*\"street\" + 0.006*\"share\"\n",
            "\n",
            "Score: 0.01667347364127636\t \n",
            "Topic: 0.007*\"health\" + 0.007*\"queensland\" + 0.006*\"fund\" + 0.006*\"election\" + 0.006*\"tuesday\" + 0.006*\"coronavirus\" + 0.006*\"federal\" + 0.005*\"budget\" + 0.005*\"update\" + 0.005*\"plan\"\n",
            "\n",
            "Score: 0.01667097955942154\t \n",
            "Topic: 0.026*\"covid\" + 0.025*\"coronavirus\" + 0.008*\"climate\" + 0.007*\"case\" + 0.006*\"change\" + 0.005*\"august\" + 0.005*\"victoria\" + 0.005*\"quarantine\" + 0.005*\"australia\" + 0.005*\"daniel\"\n",
            "\n",
            "Score: 0.01667051762342453\t \n",
            "Topic: 0.027*\"trump\" + 0.008*\"bushfire\" + 0.006*\"age\" + 0.006*\"country\" + 0.006*\"flood\" + 0.006*\"south\" + 0.006*\"rain\" + 0.006*\"storm\" + 0.005*\"coast\" + 0.005*\"bushfires\"\n",
            "\n",
            "Score: 0.016668956726789474\t \n",
            "Topic: 0.014*\"drum\" + 0.006*\"stories\" + 0.006*\"hill\" + 0.005*\"hong\" + 0.005*\"territory\" + 0.005*\"footage\" + 0.005*\"kong\" + 0.004*\"march\" + 0.004*\"drive\" + 0.004*\"whale\"\n",
            "\n",
            "Score: 0.01666880026459694\t \n",
            "Topic: 0.018*\"donald\" + 0.009*\"friday\" + 0.009*\"wednesday\" + 0.007*\"weather\" + 0.007*\"turnbull\" + 0.007*\"peter\" + 0.007*\"lockdown\" + 0.006*\"brief\" + 0.006*\"history\" + 0.005*\"smith\"\n",
            "\n",
            "Score: 0.016667725518345833\t \n",
            "Topic: 0.011*\"interview\" + 0.009*\"world\" + 0.008*\"australia\" + 0.008*\"scott\" + 0.008*\"final\" + 0.007*\"morrison\" + 0.007*\"league\" + 0.007*\"michael\" + 0.006*\"david\" + 0.006*\"cricket\"\n",
            "\n",
            "Score: 0.016667339950799942\t \n",
            "Topic: 0.010*\"government\" + 0.008*\"border\" + 0.007*\"restrictions\" + 0.006*\"coronavirus\" + 0.006*\"korea\" + 0.006*\"cattle\" + 0.005*\"say\" + 0.005*\"action\" + 0.004*\"china\" + 0.004*\"minister\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FQG7m6tKcz_R",
        "outputId": "330ea6d4-7d4b-4c06-cfa1-ef33f98c743f"
      },
      "source": [
        "documents[['headline_text','preprocessed']].values[1000]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['death toll hits 41 during bangladeshs local',\n",
              "       list(['death', 'toll', 'hit', 'bangladeshs', 'local'])],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cR6_zRO1wkFp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
